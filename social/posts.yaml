posts:
- id: adr-054-brooks-law
  title: Brooks' Law for AI Agents
  url: https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md
  x: |
    Brooks' Law applies to AI agents.

    4 agents = 6 channels
    10 agents = 45 channels

    Google/MIT measured it: exponent 1.724.

    Worse than quadratic.

    We gathered 50+ references on this.

    #AI #Agents
  linkedin: |
    Brooks' Law applies to AI agents.

    "Adding manpower to a late software project makes it later." — Fred Brooks, 1975

    The math is brutal:
    - 4 agents = 6 communication channels (manageable)
    - 10 agents = 45 channels (chaos)
    - 20 agents = 190 channels (impossible)

    Google/MIT measured it in December 2024:
    - Communication overhead scales with exponent 1.724
    - Maximum effective team size: 3-4 agents
    - Independent agents show 17.2x error amplification vs single-agent baseline

    This explains why multi-agent frameworks hit a wall.

    They fragment context across agents. Each agent gets 8-32k tokens. They coordinate through external infrastructure—state machines, message queues, databases.

    Every handoff loses information. Every coordination point adds latency. Every agent has a different view of the problem.

    The pattern that works: One large context (200k+ tokens) where the AI decides when to spawn agents.

    Context IS the coordination layer.

    No serialization. No translation errors. No "telephone game" between agents.

    We spent a week documenting this with 50+ verified references:
    https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md

    #AI #Agents #SoftwareEngineering #MultiAgent
  posted: {}

- id: adr-054-cognition-quote
  title: Devin's Creators on Multi-Agent
  url: https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md
  x: |
    "In 2025, running multiple agents only results in fragile systems."

    — Cognition (Devin's creators)

    They built the hype.
    Now they're saying don't.

    #AI #Devin
  linkedin: |
    The team behind Devin—the most hyped AI coding agent—published this:

    "In 2025, running multiple agents in collaboration only results in fragile systems. The decision-making ends up being too dispersed and context isn't able to be shared thoroughly enough between the agents."

    Their recommendation:
    1. All agents should read from the same context
    2. All agents should write to the same context

    This aligns with what the research shows:
    - Full-file context: 95% accuracy
    - Fragmented retrieval: 80% accuracy

    That 15% gap compounds over steps. By step 10, it's the difference between shipping and debugging.

    Cognition learned it the hard way. Google/MIT confirmed it. The math proves it.

    Source: https://cognition.ai/blog/dont-build-multi-agents

    We compiled the research (50+ refs):
    https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md

    #AI #Agents #Devin #SoftwareEngineering
  posted: {}

- id: adr-054-accuracy-gap
  title: 95% vs 80% - The Coherence Gap
  url: https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md
  x: |
    Full context: 95% accuracy.
    Fragmented: 80% accuracy.

    SWE-bench finding.

    The difference? Coherence.

    Full files = relationships intact.
    Fragments = stitching pieces together.

    #AI #SWEbench
  linkedin: |
    The number that should change how you think about AI agents.

    On SWE-bench-Verified:
    - Full-file context: 95% accuracy
    - Fragmented retrieval: 80% accuracy

    15 percentage points. Same task. Same model. Different architecture.

    The difference comes from coherence.

    With full files, the model sees relationships across the entire document. Variable definitions. Function calls. Import statements. The whole picture.

    With fragmented retrieval (RAG, multi-agent handoffs), the model stitches together disjointed pieces. Context is lost at every boundary. Relationships are severed.

    This is why multi-agent systems struggle with complex reasoning.

    Agent A sees the function signature.
    Agent B sees the implementation.
    Agent C sees the tests.

    None of them see the whole.

    The "Rule of 4" from Google/MIT research:
    - Maximum effective team size: 3-4 agents
    - Beyond that, coordination overhead dominates
    - 17.2x error amplification with independent agents

    The fix isn't better coordination infrastructure.

    The fix is not fragmenting context in the first place.

    Research (50+ verified refs):
    https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md

    #AI #SWEbench #Coding #SoftwareEngineering
  posted: {}

- id: adr-054-research-summary
  title: Multi-Agent Research Summary
  url: https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md
  x: |
    We spent a week on multi-agent research.

    50+ verified refs. Here's what we found:

    - 95% vs 80% accuracy (SWE-bench)
    - 17.2x error amplification (Google/MIT)
    - Max 3-4 effective agents

    The pattern that works? One context.

    #AI #Agents
  linkedin: |
    We spent a week gathering hard data on multi-agent AI architectures. 50+ verified references. Real benchmarks. Peer-reviewed research.

    The findings:

    1. Brooks' Law applies to AI agents
    Communication overhead = N × (N-1) / 2
    Google/MIT measured: exponent 1.724 (worse than quadratic)

    2. Full context crushes fragmented
    SWE-bench-Verified: 95% vs 80% accuracy
    The 15% gap is coherence—relationships across the whole, not stitched fragments

    3. The Rule of 4
    Maximum effective team size: 3-4 agents
    Beyond that, coordination overhead dominates performance gains

    4. Multi-agent token overhead
    Anthropic's own research: 15x more tokens than single-agent chat
    Token usage explains 80% of performance variance

    5. Cognition (Devin) agrees
    "In 2025, running multiple agents in collaboration only results in fragile systems."

    The implication:

    Fixed agent frameworks are engineering complexity around a fundamental limitation. They fragment context to work within smaller windows. They build coordination infrastructure to reconnect what should never have been separated.

    The pattern that works: One large context. AI decides when to spawn agents. Context IS the coordination layer.

    O(1) coordination vs O(n^1.724).

    Full research:
    https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md

    #AI #Agents #SoftwareEngineering #Research #MultiAgent
  posted: {}

- id: adr-054-rag-vs-context
  title: RAG vs Long Context - The Data
  url: https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md
  x: |
    Long context outperforms RAG by 7-13%.

    Google DeepMind, July 2024:
    - GPT-4O: +13.1%
    - Gemini-1.5-Pro: +7.6%

    RAG is 25x cheaper.

    But it fails on multi-step reasoning.

    When accuracy matters, use the full context.

    #AI #RAG #LLM
  linkedin: |
    RAG was a workaround for small context windows. The workaround is becoming obsolete.

    Google DeepMind, July 2024:
    - Long context outperforms RAG by 7-13% on average
    - GPT-4O: +13.1% with full context
    - Gemini-1.5-Pro: +7.6% with full context

    RAG failure patterns (from the research):
    1. Multi-step reasoning: Query requires previous step results for retrieval
    2. General queries: Too vague for effective retrieval
    3. Implicit queries: Requires holistic context understanding
    4. Long/complex queries: Challenging for retriever to parse

    The cost trade-off:
    - RAG: 4% of long-context cost
    - Long context: 25x more expensive

    But here's the thing:

    For code understanding, the accuracy gap is 95% vs 80% (SWE-bench).

    That 15% is the difference between shipping and debugging for hours.

    For high-value tasks—code generation, complex reasoning, multi-step analysis—the 25x cost premium pays for itself in accuracy and developer time.

    RAG still wins for:
    - Dynamic/frequently updated data
    - Cost-sensitive applications
    - Simple lookups

    But for reasoning? For code? For complex analysis?

    Use the context window.

    Original research: https://arxiv.org/abs/2407.16833

    Our analysis (50+ refs):
    https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md

    #AI #RAG #LLM #SoftwareEngineering #Research
  posted: {}

- id: error-compounding-math
  title: 5,391x - Error Compounding
  url: https://github.com/royalbit/asimov/blob/main/models/error-compounding.yaml
  x: |
    5,391x. Not a typo.

    That's the gap at 50 steps.

    Errors compound: accuracy^steps

    10 steps:
    - 80% per step → 10.7% success
    - 95% per step → 59.9% success

    We modeled this deterministically.

    #AI #Math
  linkedin: |
    The 95% vs 80% accuracy gap isn't 15 percentage points.

    Errors compound multiplicatively.

    The formula: P(success after N steps) = accuracy^N

    Here's what that means in practice:

    At 1 step:
    - 80% accuracy: 80% success
    - 95% accuracy: 95% success
    - Gap: 1.2x

    At 10 steps (typical complex task):
    - 80% accuracy: 10.7% success
    - 95% accuracy: 59.9% success
    - Gap: 5.6x

    At 20 steps (large feature):
    - 80% accuracy: 1.2% success
    - 95% accuracy: 35.8% success
    - Gap: 31x

    At 50 steps (system integration):
    - 80% accuracy: 0.001% success
    - 95% accuracy: 7.7% success
    - Gap: 5,391x

    This is why multi-agent systems collapse on complex tasks.

    Each agent handoff is a step. Each step compounds the error.

    We modeled this deterministically:
    https://github.com/royalbit/asimov/blob/main/models/error-compounding.yaml

    The math doesn't lie.

    #AI #Agents #Math #SoftwareEngineering
  posted: {}

- id: error-compounding-89-percent
  title: 89% Failure Rate
  url: https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md
  x: |
    89% failure rate.

    That's multi-agent at 10 steps.

    Full context? 40% failure.

    Same task. Different architecture.

    Each handoff compounds the error.

    #AI #Agents
  linkedin: |
    At 10 steps—a typical complex coding task—fragmented context systems have an 89% failure rate.

    Full context systems? 40% failure rate.

    Same steps. Same task. Different architecture.

    The difference is error compounding.

    When accuracy is 80% per step (fragmented/multi-agent):
    - Step 1: 80% success
    - Step 5: 32.8% success
    - Step 10: 10.7% success ← 89% failure

    When accuracy is 95% per step (full context):
    - Step 1: 95% success
    - Step 5: 77.4% success
    - Step 10: 59.9% success ← 40% failure

    This explains why multi-agent frameworks hit a wall on complex tasks.

    They fragment context. Each agent handoff is a step. Each step compounds the 20% error rate.

    By step 10, you're essentially guaranteed to fail.

    The pattern that works: One large context (200k+ tokens). AI decides when to spawn agents. Context IS the coordination layer.

    Research (50+ verified refs):
    https://github.com/royalbit/asimov/blob/main/docs/adr/054-dynamic-swarm-vs-fixed-agentic-frameworks.md

    #AI #Agents #MultiAgent #SoftwareEngineering
  posted: {}
